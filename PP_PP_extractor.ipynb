{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\" Script for extracting all prepositional phrases from SynTagRus, test subcorpus \n",
    "    (https://github.com/UniversalDependencies/UD_Russian-SynTagRus).\n",
    "    Made to match pphrase (https://github.com/merionum/pphrase) output.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Token class for easier data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    \n",
    "    #all_tokens = []\n",
    "    \n",
    "    def __init__(self, token_id=None,\n",
    "                 token=None, lemma=None,\n",
    "                 POS=None, empty=None,\n",
    "                 features=None, head=None,\n",
    "                 deprel=None, deps=None, comment=None):\n",
    "        \n",
    "        self.token_id = token_id\n",
    "        self.token = token.lower()\n",
    "        self.lemma = lemma\n",
    "        self.POS = POS\n",
    "        self.empty = empty\n",
    "        self.features = features\n",
    "        self.head = head\n",
    "        self.deprel = deprel\n",
    "        self.deps = deps\n",
    "        self.comment = comment\n",
    "        \n",
    "        #self.__class__.all_tokens.append(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing corpus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "sent = []\n",
    "\n",
    "filename = 'ru_syntagrus-ud-test.conllu'\n",
    "\n",
    "with open(filename, 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        if line == '\\n':\n",
    "            if list(filter(lambda x: x.POS == 'ADP', sent)):\n",
    "                corpus.append(tuple(sent)) # only load sentences with adpositions\n",
    "            sent = []\n",
    "        elif line.startswith('#'):\n",
    "            pass\n",
    "        else:\n",
    "            sent_data = line.strip('\\n').split('\\t')\n",
    "            tokenobj = Token(*sent_data)\n",
    "            sent.append(tokenobj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting prepositional phrases of various structural types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_constructions = []\n",
    "\n",
    "for sent_tuple in corpus:\n",
    "    for item in sent_tuple:\n",
    "        \n",
    "        # check for simple preps & multiwords starting w/prep:\n",
    "        if (item.POS == 'ADP') & (item.deprel == 'case'):\n",
    "            head_id = item.token_id # prep id\n",
    "            head_token = item.token # prep token        \n",
    "            dep_id = item.head # dependant id\n",
    "            dep_item = next((x for x in sent_tuple if x.token_id == dep_id))\n",
    "            if dep_item.deprel == 'root':\n",
    "                continue\n",
    "            dep_token = dep_item.token # dependant token        \n",
    "            host_id = dep_item.head # host id\n",
    "            host_item = next((x for x in sent_tuple if x.token_id == host_id))\n",
    "            host_token = host_item.token # host token\n",
    "\n",
    "            # check if prep is part of complex prep + form full prep:\n",
    "            is_multiword = [x for x in sent_tuple if (x.head == head_id) \n",
    "                                                    & (x.deprel == 'fixed')]\n",
    "            prep_parts = ' '.join([x.token for x in is_multiword])\n",
    "            if prep_parts:\n",
    "                prep_status = 'complex'\n",
    "                full_prep = ' '.join([head_token, prep_parts])\n",
    "            else:\n",
    "                prep_status = 'simple'\n",
    "                full_prep = head_token\n",
    "\n",
    "        # check for multiwords ending w/prep:\n",
    "        elif (item.POS == 'ADP') & (item.deprel == 'fixed'):\n",
    "            prep_token = item.token # prep token\n",
    "            head_id = item.head # multiword head id\n",
    "            head_item = next((x for x in sent_tuple if x.token_id == head_id))\n",
    "            head_token = head_item.token            \n",
    "            if head_token in ['несмотря', 'невзирая', 'вплоть', 'наравне']:\n",
    "                prep_status = 'complex'\n",
    "                full_prep = ' '.join([head_token, prep_token])\n",
    "                dep_id = head_item.head # dependant id\n",
    "                dep_item = next((x for x in sent_tuple if x.token_id == dep_id))\n",
    "                if dep_item.deprel == 'root':\n",
    "                    continue\n",
    "                dep_token = dep_item.token # dependant token        \n",
    "                host_id = dep_item.head # host id\n",
    "                host_item = next((x for x in sent_tuple if x.token_id == host_id))\n",
    "                host_token = host_item.token #host token\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # restore original order within PP:\n",
    "        phrase = []\n",
    "        phrase_ids = []\n",
    "        phrase_ids = [int(head_id), int(dep_id), int(host_id)]\n",
    "        phrase_ids.sort()\n",
    "\n",
    "        # map ids back to tokens:\n",
    "        if prep_status == 'simple':\n",
    "            for elem in phrase_ids:\n",
    "                token = [x.token for x in sent_tuple if x.token_id == str(elem)]\n",
    "                phrase += token\n",
    "\n",
    "        else:\n",
    "            for elem in phrase_ids:\n",
    "                token_item = next((x for x in sent_tuple if x.token_id == str(elem)))\n",
    "                if token_item.deprel == 'case': # if token is multiword head\n",
    "                    phrase.append(full_prep) # replace multiword head with full prep\n",
    "                else: # if token is not head\n",
    "                    phrase.append(token_item.token)\n",
    "\n",
    "        # convert ordered list constituents to pphrase format dict:   \n",
    "        phrase_str = ' '.join(phrase)\n",
    "        phrase_dict = {x: v for x, v in zip([\"phrase\", \"host\", \"prep\", \"dependant\"],\n",
    "                                            [phrase_str, host_token, full_prep, dep_token])}\n",
    "\n",
    "        #add PP to full list of PPs in corpus:\n",
    "        prep_constructions.append(phrase_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing result to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('PPs_extracted.txt', 'w+', encoding = 'utf8') as g:\n",
    "    g.write(str(prep_constructions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
